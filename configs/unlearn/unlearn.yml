dataset_args:
  dataset_path: enron_phone   # Other options: echr_year, enron_email
  setting: perturb            # Other options: noperturb, minority
  dataset_mode: undefended
  sample_duplication_rate: 1

trainer_args:
  save_steps: 3
  callback_after_n_steps: 3
  num_train_epochs: 10
  gradient_accumulation_steps: 8
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4

model_args:
  architecture: gpt2
  pre_trained: True   # Use a pre-trained checkpoint as the starting point
  # Specify the model checkpoint path for conducting unlearning
  # model_ckpt: ...
  peft: none
  operation_mode: unlearn
  unlearning_method: gradient_ascent # Other options: ascent_plus_descent, EUk, CFk, random_label, scrub
  # Uncomment for EUk and CFk unlearning methods
  # k_layers: 3

ner_args:
  ner: flair
  ner_model: flair/ner-english-ontonotes-large
  anon_token: <MASK>
  anonymize: False

outdir_args:
  # custom_output_dir: # Specify a custom output directory if necessary
